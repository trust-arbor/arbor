// Chat Quality Eval Pipeline
// Tests LLM conversational quality for the main chat interface.
//
// Usage:
//   Engine.run(graph, initial_values: %{
//     "eval.provider" => "anthropic",
//     "eval.model" => "claude-sonnet-4-5-20250929"
//   })

digraph EvalChat {
  graph [
    goal="Evaluate chat response quality for conversational use cases",
    label="Chat Quality Eval"
  ]

  start [shape=Mdiamond]

  load_dataset [
    type="exec",
    target="action",
    action="eval_pipeline.load_dataset",
    param.path="priv/eval_datasets/chat_quality.jsonl",
    data_class="internal"
  ]

  run_eval [
    type="exec",
    target="action",
    action="eval_pipeline.run_eval",
    param.subject="Arbor.Orchestrator.Eval.Subjects.LLM",
    param.graders="contains",
    context_keys="exec.load_dataset.dataset",
    data_class="internal"
  ]

  aggregate [
    type="exec",
    target="action",
    action="eval_pipeline.aggregate",
    param.metrics="accuracy,mean_score",
    context_keys="exec.run_eval.results",
    data_class="internal"
  ]

  persist [
    type="exec",
    target="action",
    action="eval_pipeline.persist",
    param.domain="chat",
    context_keys="exec.run_eval.results,exec.aggregate.metrics",
    data_class="internal"
  ]

  done [shape=Msquare]

  start -> load_dataset -> run_eval -> aggregate -> persist -> done
}
